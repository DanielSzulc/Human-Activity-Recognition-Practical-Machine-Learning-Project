---
title: "Predict a quality of exercises. Practical machine learning project"
author: "Daniel Szulc"
date: "Thursday, July 23, 2015"
output: html_document
---

## Overview

This report is prepared in line with the requirements of John Hopkins University Data Science Specialisation Practical Machine Learning Course Project.
The aim of this project was to develop a model that correctly predict how accurately did the people the weight lifting exercise. While exercising a person was wearing sensors, which collected data on the movement of different body parts (as well as dumbbell lifted).
Two different types of models were fitted. The most appropriate random forest model achieved 99% accuracy. This is an excellent result in terms of the project requirements. However, as it is discussed below, it is very unlikely to achieve such high level of accuracy on a completely new dataset.  

## Coding style

Style of code chunks follows [Hadley Wickham's Advanced R guidlines][3] with the exception of wider indentation (8 spaces).

## Loading packages
```{r message=TRUE}
# loading libraries
library(caret)
library(doParallel)
library(ggplot2)
library(rattle) # visualise CART tree
library(dplyr)
library(knitr)

# initialize Rmarkdown environment

opts_chunk$set(cache = TRUE)

# initialize parallel computing - possible in caret package
registerDoParallel()


```


## Dataset

The data come from Human Activity Recognition Project. For more information please consult [HAR webpage][1]. The Weight Lifting Exercises Dataset is described wider in [Qualitative Activity Recognition of Weight Lifting Exercises][2] paper. 

```{r download_data}
# functions --------------------------------------------------------------------
download_data <- function(file, url) {
        if(!file.exists(file)){
                
                download.file(url = url,
                              destfile=file)
                
        }  
}

# data downloading -------------------------------------------------------------
file_test <- "pml-testing.csv"
url_test <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"

file_train <- "pml-training.csv"
url_train <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"

download_data(file_test, url_test)
download_data(file_train, url_train)
```

```{r data_loading}
# loading data -----------------------------------------------------------------

training <- read.csv(file_train, stringsAsFactor=FALSE,na.string=c("NA", ""))
# there are NA and "" entries in dataset
testing <- read.csv(file_test, stringsAsFactor=FALSE,na.string=c("NA", ""))
```

After downloading and loading data into memory, we briefly inspect data to gain high-level understanding of data structure.

```{r structure_data}
str(training)
str(testing)
```

As it looks like that there is lots of NA in the data, we have to inspect this issue more closely. Let's use a combination of apply and anonymous function to select all columns in which NAs consist of more than half of observations. This function return TRUE for every column that have over 50% of NAs.

```{r NA_inspection}
columns_NAs <- apply(training,2, function(col){
        sum(is.na(col))/nrow(training)>0.5
})
sum(columns_NAs)
names(training)[columns_NAs]
```

There are exactly 100 columns that meet our >50% NAs criteria. 
It looks that these columns relate to summary measures calculated on a basis of raw measurements collected by movement sensors. These data are also missing in our testing dataset. Raw measurements are available. We have decided to delete all columns with missing values for two reasons:

1. In case of most of the columns, over 90% data are missing. It is no use imputing these data as the imputation error would be likely too large. These columns could not provide any reliable information.
2. These columns are also missing in the testing dataset, so we could not use them to predict outcomes on unknown rows.

```{r}
# removing columns with over 50% NAs
training[,names(training)[columns_NAs]]<-list(NULL)
testing[,names(testing)[columns_NAs]]<-list(NULL)
```

## Data splitting

We are to split data to training, testing and validation set in proportions 60%, 20%, 20% of rows. We decided to do such a split for two reasons:
1. There are over 19 600 rows in our dataset. It looks like a sufficient number to put aside some portion of rows (20%) and use them to estimate the accuracy of our prediction model.
2. We can use training set to fit the model and testing set to tune model parameters (or even change the model type) without running a risk to over fitting.

```{r data_splitting}
set.seed(53)
in_train <- createDataPartition(training$classe, p = 0.8, list = FALSE)
testing_cv <- training[-in_train, ]  # it will help with model selection and tuning
training <- training[in_train, ]

in_train <- createDataPartition(training$classe, p = 0.75, list = FALSE) 
# 0.75*0.8=0.6 to training
validating <- training[-in_train, ]  # we apply a model to this data just ONCE
training <- training[in_train, ]
```

## Exploratory Data Analysis

This barplot shows that class that we have to predict are pretty balanced in the training set with some advantage of A class (i.e. correctly done exercises)

```{r EDA}
# exploring training data ------------------------------------------------------
ggplot(training, aes(x = classe)) + geom_bar(fill = "dark blue") + theme_bw()

ggplot(training, aes(x = roll_belt, y = yaw_belt, color = classe)) + 
        geom_point(alpha = 0.5) + theme_bw()
```

As we can see on the above plot, the patterns in this dataset are rather non-linear and variables are not normally distributed. It suggests that non-linear models (classification trees for example) can be more appropriate than linear ones. It also points out that we should normalise (preprocess) the variables.

The table below shows how particular variables vary dependent on class.

```{r EDA_2}
classe_dependent <- select(training, roll_belt:magnet_forearm_z,classe) %>% group_by(classe) %>% summarise_each(funs(mean))  

round(t(classe_dependent[,-1]),3)

```

## Data preprocessing

```{r preprocessing}
# preprocessing ----------------------------------------------------------------
set.seed(53)
pre_proc <- preProcess(training[,8:58], method=c("center","scale"))

selected_features <- predict(pre_proc, training[, 8:58])  # only numeric variables
selected_features$classe <- factor(training$classe)

#preprocess testing (internal) set
testing_cv_preproc <- predict(pre_proc, testing_cv[,8:58])

# preprocess validation set
validating_preproc <- predict(pre_proc, validating[,8:58])
# confusionMatrix(data = predict(model, validating_feat), validating$classe)
```

We centred and scaled data and selected only numerical explanatory variables that were measured by sensors. We are not using timestamp information, user name, and similar, that would be not helpful in the real testing dataset

## Model training and evaluation

As there are many possible predictors, we are beginning with models containing all potential explanatory variables. As we discuss earlier, we expect non-linear relationships, therefore at first we try to fit models that are better in capturing such types of dependencies.


### Classification tree (CART)

```{r modelcart}
# CART using rpart method
if (file.exists("model_cart.Rds")) {
        model_cart <- readRDS("model_cart.Rds")
} else {
        model_cart <- train(classe~., data=selected_features, method="rpart")
        fancyRpartPlot(model_cart$finalModel)
        saveRDS(object = model_cart,file = "model_cart.Rds")
}

```

Apply the classification tree to the testing set.

```{r test_cart}
confusionMatrix(data = predict(model_cart, testing_cv_preproc), testing_cv$classe)
```

The below plot presents cases where the model is wrong.

```{r caret_plot}
predicted_correct <- predict(model_cart, testing_cv_preproc) == testing_cv$classe
ggplot(testing_cv_preproc, aes(x = roll_belt, y = yaw_belt, color = predicted_correct)) + geom_point(alpha = 0.5) + theme_bw()
```

The accuracy on testing set is not sufficient (below 50%). Before we try to select exploratory variables more carefully and create new features, let's try to another solution - random forest. 



### Random forest

We are beginning with default training parameters.

```{r modelrf}

if (file.exists("model_RF.Rds")) {
        model_RF <- readRDS("model_RF.Rds")
} else {
        model_RF <- train(classe~., data=selected_features, method="rf")
        saveRDS(model_RF, "model_RF.Rds")
}

```

Apply the random forest model to testing set.

```{r test_rf}
confusionMatrix(data = predict(model_RF, testing_cv_preproc), testing_cv$classe)
```

The below plot presents cases where the model is wrong.

```{r rf_plot}
predicted_correct <- predict(model_RF, testing_cv_preproc) == testing_cv$classe
ggplot(testing_cv_preproc, aes(x = roll_belt, y = yaw_belt, color = predicted_correct)) + geom_point(alpha = 0.5) + theme_bw()
```


Our model achieved almost 100% accuracy on the testing set. It looks like a very promising result. However, it can be a sing of overfitting as well. We have to apply our model to a validation set. This data were not used in any way during the training and model selecting process. The accuracy on the validation set should be a good estimation of model performance on unknown data. 

## Final validation of the model

```{r validation}
confusionMatrix(data = predict(model_RF, validating_preproc), validating$classe)
```
Our model is very accurate. We can expect accuracy around 99%. We expect very small out of sample error (around 1%). However, we should note, that such high level of accuracy could not be achievable in the case of new subjects of measurement (new people). Please note that we could use data only on 6 participants to train our model and assess its accuracy. Validation set, although not used in training and model selecting process, contains data on the same subjects and that are very close to our training dataset.

## Applying model to 20 test cases

As a part of our project, we are obliged to apply the predictive model to 20 selected test cases. Using a special function "pml_write_files",we will generate 20 separate files to submit the predictions in line with the project requirements.

```{r test_cases20}
testing_preprocessed <- predict(pre_proc, testing[,8:58])
outcome <- predict(model_RF, testing_preprocessed)


pml_write_files = function(x){
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}
pml_write_files(as.character(outcome))
```

We obtained 20/20 (100%) accuracy.



[1]: http://groupware.les.inf.puc-rio.br/har
[2]: http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201
[3]: http://adv-r.had.co.nz/Style.html
